{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc496d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a02c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3d0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c50c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938af04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f0e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0   Performance: -293.560037560002\n",
      "Episode: 1   Performance: -34.21483081813844\n",
      "Episode: 2   Performance: -400.6470162478713\n",
      "Episode: 3   Performance: -203.06309074397535\n",
      "Episode: 4   Performance: -172.92067530991196\n",
      "Episode: 5   Performance: -163.47534437112716\n",
      "Episode: 6   Performance: -229.29612142300203\n",
      "Episode: 7   Performance: 2.7261605909323094\n",
      "Episode: 8   Performance: -252.31593668445004\n",
      "Episode: 9   Performance: -146.0550995957277\n",
      "Episode: 10   Performance: -101.71053482620184\n",
      "Episode: 11   Performance: -152.0524397100358\n",
      "Episode: 12   Performance: -145.84569049219778\n",
      "Episode: 13   Performance: -237.9503321010779\n",
      "Episode: 14   Performance: -144.07934712517468\n",
      "Episode: 15   Performance: 146.07785008325976\n",
      "Episode: 16   Performance: -243.7225294973615\n",
      "Episode: 17   Performance: -238.80471284840735\n",
      "Episode: 18   Performance: -77.17386072607324\n",
      "Episode: 19   Performance: -105.40357600370334\n",
      "Episode: 20   Performance: -119.85869102584454\n",
      "Episode: 21   Performance: -107.84065686113712\n",
      "Episode: 22   Performance: -51.77239365287153\n",
      "Episode: 23   Performance: -143.97448320255003\n",
      "Episode: 24   Performance: -102.01360603805551\n",
      "Episode: 25   Performance: -149.37713152187126\n",
      "Episode: 26   Performance: -102.95375557320614\n",
      "Episode: 27   Performance: -145.27808413892933\n",
      "Episode: 28   Performance: -62.748787883593955\n",
      "Episode: 29   Performance: -131.2766334356496\n",
      "Episode: 30   Performance: -125.5806275655977\n",
      "Episode: 31   Performance: -94.64938692912635\n",
      "Episode: 32   Performance: -146.23867258624557\n",
      "Episode: 33   Performance: -109.89113185689548\n",
      "Episode: 34   Performance: -134.37138765423344\n",
      "Episode: 35   Performance: -56.30266423892921\n",
      "Episode: 36   Performance: -113.74305919808747\n",
      "Episode: 37   Performance: -75.95454436079356\n",
      "Episode: 38   Performance: -104.70320591935715\n",
      "Episode: 39   Performance: -294.9093183195108\n",
      "Episode: 40   Performance: -14.708167258874909\n",
      "Episode: 41   Performance: -258.65307658192523\n",
      "Episode: 42   Performance: -59.82946194842681\n",
      "Episode: 43   Performance: -145.90568857927104\n",
      "Episode: 44   Performance: -232.08541508347673\n",
      "Episode: 45   Performance: 202.76460915562487\n",
      "Episode: 46   Performance: -26.804826929674647\n",
      "Episode: 47   Performance: -52.62666199541535\n",
      "Episode: 48   Performance: -49.65852841582327\n",
      "Episode: 49   Performance: 11.17353972420888\n",
      "Episode: 50   Performance: 15.51742103258627\n",
      "Episode: 51   Performance: -44.45964487390715\n",
      "Episode: 52   Performance: -42.19944829054233\n",
      "Episode: 53   Performance: -27.251303536081025\n",
      "Episode: 54   Performance: -24.152214760313242\n",
      "Episode: 55   Performance: -30.843786658866804\n",
      "Episode: 56   Performance: -179.50830814487108\n",
      "Episode: 57   Performance: 11.216053518550453\n",
      "Episode: 58   Performance: -1.903708755039446\n",
      "Episode: 59   Performance: -0.06549259149876896\n",
      "Episode: 60   Performance: 158.99414113062608\n",
      "Episode: 61   Performance: 7.870600558944801\n",
      "Episode: 62   Performance: 38.503229288148034\n",
      "Episode: 63   Performance: 137.31062828013773\n",
      "Episode: 64   Performance: -19.918967411542667\n",
      "Episode: 65   Performance: -10.008951630324622\n",
      "Episode: 66   Performance: 162.0171579158508\n",
      "Episode: 67   Performance: -45.676636867703685\n",
      "Episode: 68   Performance: 267.64369095581407\n",
      "Episode: 69   Performance: 101.35900886636728\n",
      "Episode: 70   Performance: 154.61044359482185\n",
      "Episode: 71   Performance: 268.00270385177447\n",
      "Episode: 72   Performance: 264.02923680427426\n",
      "Episode: 73   Performance: -122.78012555222885\n",
      "Episode: 74   Performance: -104.59139961702694\n",
      "Episode: 75   Performance: 222.06713999757613\n",
      "Episode: 76   Performance: 204.5601526421619\n",
      "Episode: 77   Performance: -134.59175617162896\n",
      "Episode: 78   Performance: -116.04529333786701\n",
      "Episode: 79   Performance: -127.08952485970522\n",
      "Episode: 80   Performance: -127.51100630271335\n",
      "Episode: 81   Performance: -122.54501786820903\n",
      "Episode: 82   Performance: -118.48867035006441\n",
      "Episode: 83   Performance: -132.83603992483893\n",
      "Episode: 84   Performance: -94.76001256386289\n",
      "Episode: 85   Performance: -86.91929110092443\n",
      "Episode: 86   Performance: -134.8511009770491\n",
      "Episode: 87   Performance: -132.08724931750874\n",
      "Episode: 88   Performance: -127.36247040435548\n",
      "Episode: 89   Performance: 17.501469673713814\n",
      "Episode: 90   Performance: -127.92104583517767\n",
      "Episode: 91   Performance: -148.97472681544343\n",
      "Episode: 92   Performance: -141.34035551531463\n",
      "Episode: 93   Performance: -99.11747649281583\n",
      "Episode: 94   Performance: -94.75356026000004\n",
      "Episode: 95   Performance: -136.05817630590042\n",
      "Episode: 96   Performance: -128.4986969723437\n",
      "Episode: 97   Performance: -58.36895348159332\n",
      "Episode: 98   Performance: -108.19785857417322\n",
      "Episode: 99   Performance: -119.7663576609756\n",
      "Episode: 100   Performance: -106.68033259390475\n",
      "Episode: 101   Performance: -145.87033730619567\n",
      "Episode: 102   Performance: -140.35917630497025\n",
      "Episode: 103   Performance: -73.76178782491397\n",
      "Episode: 104   Performance: -122.20230336160014\n",
      "Episode: 105   Performance: -74.9549552172422\n",
      "Episode: 106   Performance: -138.30804038965348\n",
      "Episode: 107   Performance: -100.67784360247757\n",
      "Episode: 108   Performance: -133.04178746641082\n",
      "Episode: 109   Performance: -110.65204450744162\n",
      "Episode: 110   Performance: -141.76002005569364\n",
      "Episode: 111   Performance: -102.87270994311632\n",
      "Episode: 112   Performance: -130.1558686078075\n",
      "Episode: 113   Performance: -139.11761813054025\n",
      "Episode: 114   Performance: -97.82913150594581\n",
      "Episode: 115   Performance: -108.62979015165205\n",
      "Episode: 116   Performance: -31.223121138741945\n",
      "Episode: 117   Performance: -23.501999308581603\n",
      "Episode: 118   Performance: -112.5363654540858\n",
      "Episode: 119   Performance: -103.87669377765242\n",
      "Episode: 120   Performance: -96.53158997190945\n",
      "Episode: 121   Performance: -118.31995320999515\n",
      "Episode: 122   Performance: -125.68859856550785\n",
      "Episode: 123   Performance: -65.09935987347123\n",
      "Episode: 124   Performance: -94.3642951500817\n",
      "Episode: 125   Performance: -136.8963548071787\n",
      "Episode: 126   Performance: -113.1201646595064\n",
      "Episode: 127   Performance: -110.01152037327927\n",
      "Episode: 128   Performance: -15.754776769970121\n",
      "Episode: 129   Performance: -115.84814342033076\n",
      "Episode: 130   Performance: -109.13247912851591\n",
      "Episode: 131   Performance: -26.770773642688496\n",
      "Episode: 132   Performance: -144.74616068766377\n",
      "Episode: 133   Performance: -128.99085667048803\n",
      "Episode: 134   Performance: 8.995246330213506\n",
      "Episode: 135   Performance: 4.73266307272787\n",
      "Episode: 136   Performance: -21.60772872107635\n",
      "Episode: 137   Performance: 30.634755427879217\n",
      "Episode: 138   Performance: 178.52955382248498\n",
      "Episode: 139   Performance: 239.81822500540437\n",
      "Episode: 140   Performance: 14.64525358696958\n",
      "Episode: 141   Performance: -19.032773389091748\n",
      "Episode: 142   Performance: -29.14869825851631\n",
      "Episode: 143   Performance: -42.40185327732315\n",
      "Episode: 144   Performance: -50.72614865834176\n",
      "Episode: 145   Performance: -5.081230219438951\n",
      "Episode: 146   Performance: 22.16533419681133\n",
      "Episode: 147   Performance: -59.10184733160001\n",
      "Episode: 148   Performance: -40.40095455893561\n",
      "Episode: 149   Performance: 12.687988037263231\n",
      "Episode: 150   Performance: -35.524846937549725\n",
      "Episode: 151   Performance: -20.727187544171233\n",
      "Episode: 152   Performance: -58.95196434609022\n",
      "Episode: 153   Performance: -63.594062847206786\n",
      "Episode: 154   Performance: -8.934043777067378\n",
      "Episode: 155   Performance: -59.68641289720297\n",
      "Episode: 156   Performance: -71.81128009793865\n",
      "Episode: 157   Performance: 10.93513789566677\n",
      "Episode: 158   Performance: -8.921368683361825\n",
      "Episode: 159   Performance: 3.0464544213838467\n",
      "Episode: 160   Performance: -43.278713037013176\n",
      "Episode: 161   Performance: -36.244023169633294\n",
      "Episode: 162   Performance: -41.24904385768864\n",
      "Episode: 163   Performance: -29.249666616133112\n",
      "Episode: 164   Performance: 218.24468627029376\n",
      "Episode: 165   Performance: 3.8871659784542008\n",
      "Episode: 166   Performance: -17.394617896604064\n",
      "Episode: 167   Performance: 37.026424768264846\n",
      "Episode: 168   Performance: 19.515441925279934\n",
      "Episode: 169   Performance: -7.1628806026647815\n",
      "Episode: 170   Performance: 182.99925498171893\n",
      "Episode: 171   Performance: 190.8110400585134\n",
      "Episode: 172   Performance: 108.56746949362264\n",
      "Episode: 173   Performance: -181.18988274163797\n",
      "Episode: 174   Performance: 56.62103308408023\n",
      "Episode: 175   Performance: 31.407991599456732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 176   Performance: -3.5459976352660334\n",
      "Episode: 177   Performance: 84.73347509265832\n",
      "Episode: 178   Performance: 110.25230875147098\n",
      "Episode: 179   Performance: 116.0488873464355\n",
      "Episode: 180   Performance: 200.78321017531266\n",
      "Episode: 181   Performance: 192.00849624446198\n",
      "Episode: 182   Performance: 225.03041332650142\n",
      "Episode: 183   Performance: 165.57464575331588\n",
      "Episode: 184   Performance: 136.22997158954\n",
      "Episode: 185   Performance: 170.8957463592492\n",
      "Episode: 186   Performance: 19.232486573861465\n",
      "Episode: 187   Performance: 116.88772013120976\n",
      "Episode: 188   Performance: -29.280390344248968\n",
      "Episode: 189   Performance: -7.1018309647018745\n",
      "Episode: 190   Performance: -7.000596029251546\n",
      "Episode: 191   Performance: -37.935426318842474\n",
      "Episode: 192   Performance: -61.59498865513602\n",
      "Episode: 193   Performance: -53.961935446428285\n",
      "Episode: 194   Performance: 2.1296081546156227\n",
      "Episode: 195   Performance: 4.090225143900007\n",
      "Episode: 196   Performance: 204.78193316644283\n",
      "Episode: 197   Performance: -13.82429638585365\n",
      "Episode: 198   Performance: 273.84479155430427\n",
      "Episode: 199   Performance: -22.188761205012703\n",
      "Episode: 200   Performance: 217.4169083267563\n",
      "Episode: 201   Performance: -6.920333510466134\n",
      "Episode: 202   Performance: 13.945524999898922\n",
      "Episode: 203   Performance: 34.418126746152275\n",
      "Episode: 204   Performance: 34.62681637150326\n",
      "Episode: 205   Performance: 37.27178099222651\n",
      "Episode: 206   Performance: 138.65436385740736\n",
      "Episode: 207   Performance: 156.95038528235372\n",
      "Episode: 208   Performance: 41.54297176958947\n",
      "Episode: 209   Performance: 198.69151380296984\n",
      "Episode: 210   Performance: 90.77637959987315\n",
      "Episode: 211   Performance: 27.205714976731326\n",
      "Episode: 212   Performance: -197.07346935666473\n",
      "Episode: 213   Performance: 196.3201708547791\n",
      "Episode: 214   Performance: -5.485197001616929\n",
      "Episode: 215   Performance: 194.56507312500344\n",
      "Episode: 216   Performance: -56.50173307930671\n",
      "Episode: 217   Performance: 3.446217692628477\n",
      "Episode: 218   Performance: 214.43285231641642\n",
      "Episode: 219   Performance: 23.791860632851588\n",
      "Episode: 220   Performance: 184.7614657799217\n",
      "Episode: 221   Performance: 220.21262547788047\n",
      "Episode: 222   Performance: 191.43035036992148\n",
      "Episode: 223   Performance: 198.94398014651978\n",
      "Episode: 224   Performance: 219.09194706803916\n",
      "Episode: 225   Performance: 232.20571246492506\n",
      "Episode: 226   Performance: 213.5005010557134\n",
      "Episode: 227   Performance: 249.57710047243344\n",
      "Episode: 228   Performance: 234.68187627883907\n",
      "Episode: 229   Performance: 225.53985969981434\n",
      "Episode: 230   Performance: 220.78119225860092\n",
      "Episode: 231   Performance: 224.45579547467378\n",
      "Episode: 232   Performance: 199.21587103021227\n",
      "Episode: 233   Performance: 246.3347101490752\n",
      "Episode: 234   Performance: 266.30641779965003\n",
      "Episode: 235   Performance: 198.3316960279485\n",
      "Episode: 236   Performance: 208.1266957626242\n",
      "Episode: 237   Performance: 46.80333189121804\n",
      "Episode: 238   Performance: 172.10241895750386\n",
      "Episode: 239   Performance: 12.241388455987643\n",
      "Episode: 240   Performance: 192.15949461048984\n",
      "Episode: 241   Performance: 254.51863486689496\n",
      "Episode: 242   Performance: 246.4939172032752\n",
      "Episode: 243   Performance: 225.32833649253132\n",
      "Episode: 244   Performance: 225.39450981782085\n",
      "Episode: 245   Performance: 164.13365550847305\n",
      "Episode: 246   Performance: 197.62092670969923\n",
      "Episode: 247   Performance: -15.377681129642465\n",
      "Episode: 248   Performance: 213.62551356006443\n",
      "Episode: 249   Performance: 227.34278303831343\n",
      "Episode: 250   Performance: 178.75716136703477\n",
      "Episode: 251   Performance: 203.01417724083038\n",
      "Episode: 252   Performance: 237.90293244595787\n",
      "Episode: 253   Performance: 206.8890140977009\n",
      "Episode: 254   Performance: 210.67466735329867\n",
      "Episode: 255   Performance: 239.61662431101504\n",
      "Episode: 256   Performance: 206.2112009483608\n",
      "Episode: 257   Performance: 236.21606819275485\n",
      "Episode: 258   Performance: 252.93999121189816\n",
      "Episode: 259   Performance: 217.97288847889894\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8517/2388796611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Soft update of the target network's weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8517/3034530761.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# In-place gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m                   \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                   \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                   capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    230\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# Use the max. for normalizing running avg. of gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 600\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards = []\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    print(f'Episode: {i_episode}   Performance: {sum(rewards)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae006a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"LunarLander_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59b4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def make_action(observation):\n",
    "    action = policy_net(torch.tensor([observation])).max(1)[1]\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f338513",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(2000):\n",
    "#    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   action = make_action(observation)\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "        break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503ac2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
