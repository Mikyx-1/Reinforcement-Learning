{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc7326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5c7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d333b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_observations, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.l1(x))\n",
    "        x = nn.ReLU()(self.l2(x))\n",
    "        out = nn.Tanh()(self.l3(x))*2.\n",
    "        return out\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_observations + 1, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        x = torch.cat([states, actions], dim=1)\n",
    "        x = nn.ReLU()(self.l1(x))\n",
    "        x = nn.ReLU()(self.l2(x))\n",
    "        return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b2c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47bc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs, _ = env.reset()\n",
    "n_observations = len(obs)\n",
    "\n",
    "## Algorithm\n",
    "PolicyNet = Policy(n_observations)\n",
    "PolicyTarget = Policy(n_observations)\n",
    "PolicyTarget.load_state_dict(PolicyNet.state_dict())\n",
    "\n",
    "QNet = QNetwork(n_observations)\n",
    "QNetTarget = QNetwork(n_observations)\n",
    "QNetTarget.load_state_dict(QNet.state_dict())\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "LR = 1e-3\n",
    "QNetLossFn = nn.HuberLoss()\n",
    "num_epochs = 1000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.95\n",
    "TAU = 0.005\n",
    "memory = ReplayBuffer(10000)\n",
    "steps_done = 0\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "\n",
    "PolicyOptimizer = optim.AdamW(PolicyNet.parameters(), lr=LR, amsgrad=True)\n",
    "QNetOptimizer = optim.AdamW(QNet.parameters(), lr=LR, amsgrad = True)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END)*math.exp(-1.*steps_done/EPS_DECAY)\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return (PolicyNet(state) + torch.tensor(noise())).clamp_(-2.0, 2.0).view(1).float()\n",
    "    return torch.tensor(env.action_space.sample(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e95ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END)*math.exp(-1.*steps_done/EPS_DECAY)\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return (PolicyNet(state)).clamp_(-2.0, 2.0).view(1).float()\n",
    "    return torch.tensor(env.action_space.sample(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2330d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(0.2)*np.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ba4de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).unsqueeze(1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_actions = PolicyTarget(next_state_batch)\n",
    "        y = reward_batch.unsqueeze(1) + GAMMA*QNetTarget(next_state_batch, target_actions)\n",
    "        \n",
    "    critic_value = QNet(state_batch, action_batch)\n",
    "    critic_loss = nn.MSELoss()(critic_value, y)\n",
    "    QNet.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(QNet.parameters(), 80)\n",
    "    QNetOptimizer.step()\n",
    "    \n",
    "    actions = PolicyNet(state_batch)\n",
    "    policy_loss = -torch.mean(QNet(state_batch, actions))\n",
    "    PolicyNet.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(PolicyNet.parameters(), 80)\n",
    "    PolicyOptimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62c37fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0        Performance: -130.75235965651683\n",
      "Episode: 1        Performance: -128.2783305060108\n",
      "Episode: 2        Performance: -124.92718055044442\n",
      "Episode: 3        Performance: -247.3255717136473\n",
      "Episode: 4        Performance: -126.98587372977205\n",
      "Episode: 5        Performance: -236.72478527270025\n",
      "Episode: 6        Performance: -115.65795082505434\n",
      "Episode: 7        Performance: -123.81054876290732\n",
      "Episode: 8        Performance: -129.05448260648572\n",
      "Episode: 9        Performance: -126.63289007403576\n",
      "Episode: 10        Performance: -131.4783462693548\n",
      "Episode: 11        Performance: -128.96816383821445\n",
      "Episode: 12        Performance: -117.05128016517159\n",
      "Episode: 13        Performance: -2.7793778933161746\n",
      "Episode: 14        Performance: -254.85156306103437\n",
      "Episode: 15        Performance: -123.91436979152647\n",
      "Episode: 16        Performance: -114.86524998769261\n",
      "Episode: 17        Performance: -243.69502915480447\n",
      "Episode: 18        Performance: -132.27421954566466\n",
      "Episode: 19        Performance: -128.50661256767938\n",
      "Episode: 20        Performance: -124.96303902923684\n",
      "Episode: 21        Performance: -127.0511067211032\n",
      "Episode: 22        Performance: -250.84694417135324\n",
      "Episode: 23        Performance: -129.13710549746898\n",
      "Episode: 24        Performance: -120.16143878369095\n",
      "Episode: 25        Performance: -305.5627179172178\n",
      "Episode: 26        Performance: -241.6308986691425\n",
      "Episode: 27        Performance: -241.6402267759139\n",
      "Episode: 28        Performance: -256.53676140329844\n",
      "Episode: 29        Performance: -119.04969243493842\n",
      "Episode: 30        Performance: -123.41734237929634\n",
      "Episode: 31        Performance: -127.6276044346776\n",
      "Episode: 32        Performance: -126.18709281565188\n",
      "Episode: 33        Performance: -117.93965671454457\n",
      "Episode: 34        Performance: -128.0357071969009\n",
      "Episode: 35        Performance: -1.690207792976342\n",
      "Episode: 36        Performance: -115.97014442239364\n",
      "Episode: 37        Performance: -120.18911596303566\n",
      "Episode: 38        Performance: -124.29038726340787\n",
      "Episode: 39        Performance: -116.00088961948798\n",
      "Episode: 40        Performance: -232.58340298974235\n",
      "Episode: 41        Performance: -127.41980169728892\n",
      "Episode: 42        Performance: -240.10181810554442\n",
      "Episode: 43        Performance: -128.21937938360273\n",
      "Episode: 44        Performance: -232.28742833322931\n",
      "Episode: 45        Performance: -237.52348270079824\n",
      "Episode: 46        Performance: -0.640219301347775\n",
      "Episode: 47        Performance: -129.08562393486133\n",
      "Episode: 48        Performance: -122.16965367134607\n",
      "Episode: 49        Performance: -128.64002375680442\n",
      "Episode: 50        Performance: -0.8406114645016174\n",
      "Episode: 51        Performance: -1.0405681138760094\n",
      "Episode: 52        Performance: -2.8575926106173277\n",
      "Episode: 53        Performance: -241.66362930745586\n",
      "Episode: 54        Performance: -239.60241873220903\n",
      "Episode: 55        Performance: -117.50688088070353\n",
      "Episode: 56        Performance: -4.274553477976022\n",
      "Episode: 57        Performance: -242.41664064154685\n",
      "Episode: 58        Performance: -255.49370071402157\n",
      "Episode: 59        Performance: -5.485551118155975\n",
      "Episode: 60        Performance: -353.09034268851167\n",
      "Episode: 61        Performance: -129.04480609727526\n",
      "Episode: 62        Performance: -385.7396689878285\n",
      "Episode: 63        Performance: -130.43839223067675\n",
      "Episode: 64        Performance: -253.2000203934922\n",
      "Episode: 65        Performance: -135.6889723810629\n",
      "Episode: 66        Performance: -126.30711255385208\n",
      "Episode: 67        Performance: -129.91034454879855\n",
      "Episode: 68        Performance: -371.8287578646412\n",
      "Episode: 69        Performance: -137.98909804690143\n",
      "Episode: 70        Performance: -130.75791835670682\n",
      "Episode: 71        Performance: -115.13404189636562\n",
      "Episode: 72        Performance: -252.74534515631194\n",
      "Episode: 73        Performance: -131.53505999171205\n",
      "Episode: 74        Performance: -249.22453910885343\n",
      "Episode: 75        Performance: -129.18962244803052\n",
      "Episode: 76        Performance: -130.047113635207\n",
      "Episode: 77        Performance: -245.01016653422982\n",
      "Episode: 78        Performance: -127.36376708430646\n",
      "Episode: 79        Performance: -116.22974817634032\n",
      "Episode: 80        Performance: -129.4103564223638\n",
      "Episode: 81        Performance: -122.51052649458279\n",
      "Episode: 82        Performance: -244.8191757847642\n",
      "Episode: 83        Performance: -239.581753084311\n",
      "Episode: 84        Performance: -122.66756656138051\n",
      "Episode: 85        Performance: -126.71027739043892\n",
      "Episode: 86        Performance: -123.25375632107702\n",
      "Episode: 87        Performance: -234.8803586762526\n",
      "Episode: 88        Performance: -131.586908989879\n",
      "Episode: 89        Performance: -234.3313095155842\n",
      "Episode: 90        Performance: -364.5414354858207\n",
      "Episode: 91        Performance: -3.3503787286159095\n",
      "Episode: 92        Performance: -245.92486973703132\n",
      "Episode: 93        Performance: -122.55714663336994\n",
      "Episode: 94        Performance: -250.3522606146023\n",
      "Episode: 95        Performance: -246.8194196189335\n",
      "Episode: 96        Performance: -126.84903544143286\n",
      "Episode: 97        Performance: -127.37407826669487\n",
      "Episode: 98        Performance: -2.8131943072712553\n",
      "Episode: 99        Performance: -130.5741789938621\n",
      "Episode: 100        Performance: -121.5480287518915\n",
      "Episode: 101        Performance: -244.8391002611316\n",
      "Episode: 102        Performance: -130.1489040528614\n",
      "Episode: 103        Performance: -131.91223043700282\n",
      "Episode: 104        Performance: -264.64480619377235\n",
      "Episode: 105        Performance: -125.53591812755627\n",
      "Episode: 106        Performance: -130.71304404872365\n",
      "Episode: 107        Performance: -134.00295575864482\n",
      "Episode: 108        Performance: -129.98228588899082\n",
      "Episode: 109        Performance: -121.61773976347571\n",
      "Episode: 110        Performance: -118.92083295495357\n",
      "Episode: 111        Performance: -124.75321383068606\n",
      "Episode: 112        Performance: -120.6282789873775\n",
      "Episode: 113        Performance: -239.71000401509937\n",
      "Episode: 114        Performance: -248.94149754402287\n",
      "Episode: 115        Performance: -254.0534823574087\n",
      "Episode: 116        Performance: -229.4301516148014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6792/2461477377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpolicy_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6792/2570775715.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mPolicyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mPolicyOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virenv1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "for i_episode in range(num_epochs):\n",
    "    rewards = []\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32).view(1, 3)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "        rewards.append(reward)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32).view(1, 3)\n",
    "            \n",
    "        memory.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        optimize_model()\n",
    "        \n",
    "        policy_state_dict = PolicyNet.state_dict()\n",
    "        policy_target_state_dict = PolicyTarget.state_dict()\n",
    "        QNet_state_dict = QNet.state_dict()\n",
    "        QNetTarget_state_dict = QNetTarget.state_dict()\n",
    "        \n",
    "        for key in policy_state_dict:\n",
    "            policy_target_state_dict[key] = TAU*policy_state_dict[key] + (1-TAU)*policy_target_state_dict[key]\n",
    "        PolicyTarget.load_state_dict(policy_target_state_dict)\n",
    "        \n",
    "        for key in QNet_state_dict:\n",
    "            QNetTarget_state_dict[key] = TAU*QNet_state_dict[key] + (1-TAU)*QNetTarget_state_dict[key]\n",
    "        QNetTarget.load_state_dict(QNetTarget_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode: {i_episode}        Performance: {sum(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b01a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def make_decision(state):   # Already in torch format\n",
    "    action = PolicyNet(state)\n",
    "    action = action.numpy()[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a67018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-128.15198121892035\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode = \"human\")\n",
    "state, _ = env.reset()\n",
    "rewards = []\n",
    "state = torch.tensor(state).view(1, 3)\n",
    "for _ in range(100000):\n",
    "    action = make_decision(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    state = torch.tensor(state).view(1, 3)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(sum(rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e291c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "472fad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(PolicyNet.state_dict(), \"PolicyNet_PendulumV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0703a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv1",
   "language": "python",
   "name": "youtube-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
